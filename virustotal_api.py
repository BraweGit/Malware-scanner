import requests
import threading
import ntpath
import time
import os

class VirusTotalApi:
    
    def __init__(self, apiKey = "45bede5e377846b212a596c835271e98b0cb764a1f102c0c916a0a4d9f47cb2e"):
        self.ApiKey = apiKey
        self.cancel_request = False
        self.batch_size = 4 #4
        self.interval = 90 #70
        self.max_retry = 2
        self.max_file_size = 31000000

    def get_response(self, md5):
        """Gets reponse from the VirusTotal API.

        Args:
            md5(string): String of MD5 hash of file to scan.
        
        Returns:
            dictionary: Response for provided MD5 hash from the VirusTotal API.
        """ 
        try:
            params = {'apikey': self.ApiKey, 'resource': md5}
            headers = {"Accept-Encoding": "gzip, deflate",
            "User-Agent" : "gzip,  My Python requests library example client or username"}

            response = requests.get('https://www.virustotal.com/vtapi/v2/file/report', params=params, headers=headers)

            return response.json()
        except ValueError:
            print("Error! Unable to process API request!")

        

    def get_file_size(self, file):
        st = os.stat(file)
        return st.st_size


    def send_file_request(self, file):

        try:
            if self.get_file_size(file) < self.max_file_size:
                params = {'apikey': self.ApiKey}
                files = {'file': (ntpath.basename(file), open(file, 'rb'))}
                requests.post('https://www.virustotal.com/vtapi/v2/file/scan', files=files, params=params)
        except ValueError:
            print("Error! Unable to send file to the API!")

        

    def upload_files(self, files):
        
        for file in files:
            self.send_file_request(file)
            #Wait for 90 seconds for next request.
            time.sleep(self.interval)


    def get_batch_response(self, md5_list, db_callback):

        if len(md5_list) == 0:
            return None

        new_records = []
        max_batch_size = self.batch_size
        #Go through every hash.
        #And put them into groups by 4 (max size of the request).
        for i in range(0, len(md5_list), self.batch_size): 

            csv_request = ','.join(md5_list[i:max_batch_size])

            #Try to send request max max_retry times.
            for j in range(self.max_retry):
                response = self.get_response(csv_request)

                if len(md5_list) == 1:
                    response_len = 1
                else:
                    response_len = len(response)

                #If we got response for all hashes,
                #we dont need any more requests.
                if response_len == len(md5_list[i:self.batch_size]):
                    break
                else:
                    #Wait for 90 seconds for next request.
                    time.sleep(self.interval)

            max_batch_size = max_batch_size + self.batch_size

            if isinstance(response, dict):
                new_records.append(response)
                db_callback(new_records)
            else:
                new_records.extend(response)
                db_callback(new_records)
        #print(len(out))
            